---
title: "Practical Machine Learning Project Writeup"

author: "Celso Castro"
date: "Sunday, July 26, 2015"
output: html_document

---
```{r lib, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
library(AppliedPredictiveModeling)
library(data.table)
library(lattice)
library(ggplot2)
library(caret)
library(randomForest)
library(rattle)
library(RColorBrewer)
library(rpart) 
library(rpart.plot)
library(tree)
```

#OBJECTIVE
Create a report that explains how a model to predict the manner in which a subject  do its excersicse is built, showing how the cross validation mas made and what are the assumptions established.

The training data for this project IS available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv and the test data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: [http://groupware.les.inf.puc-rio.br/har]. The next paragrap is extracted from the document.

"Participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in 5 different fashions:
* Exactly according to the specification (Class A)
* Throwing the elbows to the front (Class B)
* Lifting the dumbbell only halfway (Class C)
* Lowering the dumbbell only halfway (Class D)
* throwing the hips to the front (Class E)

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes." 

The goal is to buil a prediction model based on minimizing the out fo sample error with the best possible accuracy. Two models are tested using random forest algorithms. **The previous categories will be used.**

#DATASET
The dataset contains 19,622 observations and 160 variables describing characteristics useful to predict the form in which 6 subjects made a barber lift. There is just one way to do this correctly and four categories for ech type of commomn mistake.

The dataset needs to be cleaned because there are variables that are not useful to build the prediction model, more important, they can distorsionate the result.

First, after reading the data the variables that contains missing values will be eliminated from the sample.Then it is going to be analized if there are variables with unique or few values (near zero predictors.)

```{r clean, fig.align='center'}
set.seed(1890)
TrainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
TestUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
traindata<- fread(TrainUrl)
testdata <- fread(TestUrl)

Nas <- sapply(testdata, function (x) any(is.na(x) | x == ""))
isPredictor <- !Nas & grepl("belt|[^(fore)]arm|dumbbell|forearm", names(Nas))
varnona <- names(Nas)[isPredictor]
predvar<-c("classe",varnona)

traindata<-traindata[,predvar,with=FALSE]
traindata[,.N,classe] #show elemens by factor

plot(as.factor(traindata$classe),col="blue4", main="Training data set by Classe Levels", xlab="classe levels", ylab="Frequency")

```

#SIZE TRAIN AND TEST SETS
Once the data is clean the sizes for the training and testing sets are set being 60% and 40& respectively.

the values of the predictors are centered and scaled with the objective of ensure that the criterion for choosing successive factors is based on how much variation they explain (same units).

```{r clean center}
trainpart <- createDataPartition(traindata$classe, p=0.6)
trainpredict <- traindata[trainpart[[1]]]
testpredict <- traindata[-trainpart[[1]] ]

vars <- trainpredict[, varnona, with=FALSE]
preProc <- preProcess(vars)
preProc
censcalvars <- predict(preProc, vars)
csdatatrain <- data.table(data.frame(classe = trainpredict[, classe], censcalvars ))

vars <- testpredict[, varnona, with=FALSE]
censcalvars <- predict(preProc, vars)
csdatatest <- data.table(data.frame(classe = testpredict[, classe], censcalvars ))
```

To test wich algorithm is best (Decision Three or Random Forest) their prediction accuracy is calculated. This is the mechanism to prove **cross-validation**.

```{r TestAcc,fig.align='center'}
Alg1 <- rpart(classe ~ ., data=csdatatrain, method="class")
fancyRpartPlot(Alg1)
Alg1pred <- predict(Alg1, csdatatest, type = "class")
confusionMatrix(Alg1pred, csdatatest$classe)
tree.training=tree(classe~.,data=csdatatrain)
plot(tree.training, main="Classification Tree")
text(tree.training,pretty=1, cex =.6)
Alg2<-randomForest(classe~., data = csdatatrain)
Alg2pred <- predict(Alg2, csdatatest, type = "class")
confusionMatrix(Alg2pred, csdatatest$classe)
varImpPlot(Alg2,)
```

The randomForest prediction model is selected because it have an **accuracy** of **98.57%** over the **71.77%** of the Decision Three.

The **expected out-of-sample error** for this model is **1.43%.**

Using the test data the predictions are calculated
```{r predic}
Predictions<-predict(Alg2, testdata, type = "class")
```
#REFERENCES
[1] Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.
